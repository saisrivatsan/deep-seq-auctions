{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fb7da-4ac2-4a61-b86d-24cb38cac15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces\n",
    "\n",
    "from utils import init_logger, evaluate_policy, make_env\n",
    "import bundle\n",
    "import entryfee\n",
    "from buffer import RolloutBuffer\n",
    "from net import ActorCriticNetworkBundle, ActorCriticNetworkEntryFee\n",
    "from fpi import FPI, FPIScale\n",
    "from distribution import UNIF, ASYM, COMB1, COMB2, UNIFScale\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Hyper-parameters \"\"\"\n",
    "class Args:\n",
    "       \n",
    "    \"\"\" Env Params ------------------------------ \"\"\"\n",
    "    \n",
    "    env_type: str = \"\"\n",
    "    \"\"\" Environment Type (To be filled later) \"\"\"\n",
    "\n",
    "    num_agents: int = 0\n",
    "    \"\"\"Number of agents (To be filled later) \"\"\"\n",
    "\n",
    "    num_items: int = 0\n",
    "    \"\"\"Number of items (To be filled later) \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" Policy Params ---------------------------- \"\"\"\n",
    "    \n",
    "    log_std_init: float = -2\n",
    "    \"\"\"std for exploration\"\"\"\n",
    "    \n",
    "    num_hidden_units: int = 256\n",
    "    \"\"\" Number of hidden units\"\"\"\n",
    "    \n",
    "    num_hidden_layers: int = 3\n",
    "    \"\"\" Number of hidden layers \"\"\"\n",
    "    \n",
    "    d_model: int = 12\n",
    "    \"\"\" Positional Embedding Dimensions \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\" Optimization Params ---------------------- \"\"\"\n",
    "    \n",
    "    lr_vf: float = 1e-3\n",
    "    lr_pi: float = 1e-4\n",
    "    \"\"\"Learning Rate for value fitting and policy improvement\"\"\"\n",
    "    \n",
    "    vf_epochs: int = 200\n",
    "    pi_epochs: int = 50\n",
    "    \"\"\" Epochs for value fitting and policy improvement \"\"\"\n",
    "    \n",
    "    batch_size: int = 256\n",
    "    \"\"\" Minibatch size \"\"\"\n",
    "    \n",
    "    num_envs: int = 1024\n",
    "    \"\"\" Number of parallel environments \"\"\"\n",
    "        \n",
    "    gamma: float = 1.0\n",
    "    \"\"\" Discount Factor \"\"\"\n",
    "    \n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\" GAE lambda \"\"\"\n",
    "    \n",
    "    tau: float = 100\n",
    "    \"\"\" Softmax temperature \"\"\"\n",
    "    \n",
    "    num_samples_for_pi: int = 256\n",
    "    \"\"\" Number of samples to estimate gradient in policy improvement step \"\"\"\n",
    "    \n",
    "    log_std_decay: float = 0.25\n",
    "    \"\"\" How much to decay log_std after every iteration \"\"\"\n",
    "    \n",
    "    max_iteration: int = 10\n",
    "    \"\"\" Max iteration \"\"\"\n",
    "\n",
    "    \n",
    "    \"\"\" Miscellaneous Params --------------------- \"\"\"\n",
    "    \n",
    "    device: str = \"cuda\"\n",
    "    \"\"\" CUDA or CPU \"\"\"\n",
    "    \n",
    "    t_max: int = 8 * 60 * 60\n",
    "    \"\"\" Max time to train: 8 hrs \"\"\"\n",
    "    \n",
    "    print_iter: int = 100\n",
    "    \"\"\" When to log stats \"\"\"\n",
    "    \n",
    "    seed: int = 24\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Set hyper-params \"\"\"\n",
    "args = Args()\n",
    "args.num_agents = 50\n",
    "args.num_items = 50\n",
    "args.env_type = \"unif\"\n",
    "\n",
    "\"\"\" Environment Type \"\"\"\n",
    "if args.num_items <= 10:\n",
    "    if args.env_type == \"unif\":\n",
    "        v_dist = UNIF(args.num_items, demand = None)\n",
    "        \n",
    "    elif args.env_type == \"unit\":\n",
    "        v_dist = UNIF(args.num_items, demand = 1)\n",
    "\n",
    "    elif args.env_type == \"3demand\":\n",
    "        v_dist = UNIF(args.num_items, demand = 3)\n",
    "\n",
    "    elif args.env_type == \"asym\":\n",
    "        v_dist = ASYM(args.num_items, demand = None)\n",
    "\n",
    "    elif args.env_type == \"comb1\":\n",
    "        v_dist = COMB1(args.num_items, demand = None)\n",
    "\n",
    "    elif args.env_type == \"comb2\":\n",
    "        v_dist = COMB2(args.num_items, demand = None)\n",
    "    \n",
    "    else:\n",
    "        print(\"Auction Env not supported\")\n",
    "        exit(1)\n",
    "        \n",
    "    env_class = bundle.AuctionEnv\n",
    "    policy_class = ActorCriticNetworkBundle\n",
    "    model_class = FPI\n",
    "\n",
    "elif args.env_type == \"unif\":\n",
    "    v_dist = UNIFScale(args.num_items, demand = None)\n",
    "    \n",
    "    args.td_epochs = 200\n",
    "    args.pi_epochs = 50\n",
    "    \n",
    "    env_class = entryfee.AuctionEnv\n",
    "    policy_class = ActorCriticNetworkEntryFee\n",
    "    model_class = FPIScale\n",
    "    \n",
    "else:\n",
    "    print(\"Auction Env not supported\")\n",
    "    exit(1)\n",
    "\n",
    "v_dist.set_action_scale([1.0])\n",
    "\n",
    "\n",
    "\"\"\" Loggers \"\"\"\n",
    "log_fname = os.path.join(\"experiments\", \"FPI\", \"%s_%dx%d\"%(args.env_type, args.num_agents, args.num_items))\n",
    "logger = init_logger(log_fname)\n",
    "\n",
    "\n",
    "\"\"\" Seed for reproducibility \"\"\"\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env(env_class, args.num_agents, args.num_items, v_dist) for i in range(args.num_envs)])\n",
    "eval_envs = gym.vector.SyncVectorEnv([make_env(env_class, args.num_agents, args.num_items, v_dist) for i in range(args.num_envs)])\n",
    "agent = policy_class(envs, args.num_hidden_layers, args.num_hidden_units, args.d_model, args.num_agents + 1, args.log_std_init).to(args.device)\n",
    "rollout_buffer = RolloutBuffer(envs, args.num_agents, args.gamma, args.gae_lambda, args.device)\n",
    "model = model_class(envs, agent, rollout_buffer, args, v_dist)   \n",
    "    \n",
    "\n",
    "\"\"\" Train \"\"\"\n",
    "tic = time.time()\n",
    "for iteration in range(args.max_iteration):\n",
    "    model.learn()\n",
    "    t = time.time() - tic\n",
    "    rev_eval = evaluate_policy(agent, envs, num_eval_episodes = 10240)\n",
    "    logger.info(\"[Iter]: %d, [Time Elapsed]: %.4f, [Rev]: %.6f\"%(iteration + 1, t, rev_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da152f6f-1f65-4ea4-9629-19fd87c6736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "for iteration in range(args.max_iteration):\n",
    "    model.learn()\n",
    "    t = time.time() - tic\n",
    "    rev_eval = evaluate_policy(agent, envs, num_eval_episodes = 10240)\n",
    "    logger.info(\"[Iter]: %d, [Time Elapsed]: %.4f, [Rev]: %.6f\"%(iteration + 1, t, rev_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de4294-952e-41a4-94fd-e8ffe55ac728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-spem]",
   "language": "python",
   "name": "conda-env-.conda-spem-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
